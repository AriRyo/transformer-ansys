{"cells":[{"cell_type":"markdown","metadata":{"id":"M1oqh0F6W3ad"},"source":["# FomulaBEAT\n","\n","変更点\n","- 低次元で学習させる\n","- QKVについてもっと詳しく分析する\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["version = '02-1'\n","model_dir = './model/' + version\n","data_path = 'data/eq02.txt'"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2860,"status":"ok","timestamp":1611303247694,"user":{"displayName":"Karan Sonawane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64","userId":"05479461208077736330"},"user_tz":-330},"id":"IMnymRDLe0hi","outputId":"706de1c8-715a-41e2-bdcf-3caa67125bf8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/arifuku/ymmtlab/TransformerAnsys/.venv/lib/python3.10/site-packages/torchtext/data/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/home/arifuku/ymmtlab/TransformerAnsys/.venv/lib/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/home/arifuku/ymmtlab/TransformerAnsys/.venv/lib/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"]}],"source":["from pathlib import Path\n","import math\n","import time\n","from collections import Counter\n","from tqdm import tqdm\n","import torch\n","from torch.utils.data import random_split\n","import torch.nn as nn\n","from torch import Tensor\n","from torch.nn import (\n","    TransformerEncoder, TransformerDecoder,\n","    TransformerEncoderLayer, TransformerDecoderLayer\n",")\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import vocab\n","from torchtext.utils import download_from_url, extract_archive"]},{"cell_type":"markdown","metadata":{},"source":["パラメータの事前設定"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","torch.set_printoptions(linewidth=100)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","model_dir_path = Path(model_dir)\n","if not model_dir_path.exists():\n","    model_dir_path.mkdir(parents=True)"]},{"cell_type":"markdown","metadata":{},"source":["データの取得"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def read_equation_file(file_path):\n","    with open(file_path, 'r') as file:\n","        lines = file.readlines()\n","    src_data, tgt_data = [], []\n","    for line in lines:\n","        src, tgt = line.strip().split('=')\n","        src_data.append(src)\n","        tgt_data.append(tgt)\n","    return src_data, tgt_data\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['8+0', '5+2', '5+1'] ['8', '7', '6']\n"]}],"source":["# ファイルを読み込み、数式データを取得\n","src_data, tgt_data = read_equation_file(data_path)\n","print(src_data[:3], tgt_data[:3])\n"]},{"cell_type":"markdown","metadata":{},"source":["辞書データの作成"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["\n","SPECIALS = ['<unk>', '<pad>', '<start>', '<end>']\n","\n","def build_vocab(texts):\n","    vocab = {}\n","    idx = 0\n","    # 数字の語彙定義\n","    for i in range(10):\n","        vocab[str(i)] = idx\n","        idx += 1\n","    # 特別語の語彙定義\n","    for sp in SPECIALS:\n","        vocab[sp] = idx\n","        idx += 1\n","    # その他の文字の語彙定義\n","    for text in texts:\n","        for char in text:\n","            if char not in vocab:\n","                vocab[char] = idx\n","                idx += 1\n","    return vocab\n","\n","\n","def convert_text_to_indexes(text, vocab):\n","    # <start> と <end> トークンを追加して数値化\n","    return [vocab['<start>']] + [vocab[char] if char in vocab else vocab['<unk>'] for char in text] + [vocab['<end>']]\n","\n","# データを処理して Train と Valid に分ける関数\n","# データを処理して Train と Valid に分ける関数\n","def data_process_split(src_texts, tgt_texts, vocab_src, vocab_tgt, valid_size=0.2):\n","    # データを数値化\n","    data = []\n","    for (src, tgt) in zip(src_texts, tgt_texts):\n","        src_tensor = torch.tensor(convert_text_to_indexes(src, vocab_src), dtype=torch.long)\n","        tgt_tensor = torch.tensor(convert_text_to_indexes(tgt, vocab_tgt), dtype=torch.long)\n","        data.append((src_tensor, tgt_tensor))\n","    \n","    # データのサイズを計算して、訓練データと検証データに分割\n","    data_size = len(data)\n","    valid_size = int(valid_size * data_size)\n","    train_size = data_size - valid_size\n","\n","    # PyTorchのrandom_splitを使って分割\n","    train_data, valid_data = random_split(data, [train_size, valid_size])\n","    \n","    return train_data, valid_data\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '<unk>': 10, '<pad>': 11, '<start>': 12, '<end>': 13}\n"]}],"source":["# 辞書と逆辞書を構築\n","vocab_src = build_vocab(src_data)\n","vocab_tgt = build_vocab(tgt_data)\n","\n","print(vocab_tgt)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["インデックス化された文章\n","Input: tensor([12,  7, 14,  8, 13])\n","Output: tensor([12,  1,  5, 13])\n","元に戻した文章\n","Input: 7+8\n","Output: 15\n"]}],"source":["\n","# データを数値化\n","train_data, valid_data = data_process_split(src_data, tgt_data, vocab_src, vocab_tgt)\n","\n","# 結果の確認\n","print('インデックス化された文章')\n","print(f\"Input: {train_data[0][0]}\\nOutput: {train_data[0][1]}\")\n","\n","# インデックスから元の文字列に戻す\n","def convert_indexes_to_text(indexes, vocab):\n","    reverse_vocab = {idx: token for token, idx in vocab.items()}\n","    indexes = indexes.tolist()\n","    return ''.join([reverse_vocab[idx] for idx in indexes if idx in reverse_vocab and reverse_vocab[idx] not in ['<start>', '<end>', '<pad>']])\n","\n","print('元に戻した文章')\n","print(f\"Input: {convert_indexes_to_text(train_data[0][0], vocab_src)}\")\n","print(f\"Output: {convert_indexes_to_text(train_data[0][1], vocab_tgt)}\")\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["batch_size = 128\n","PAD_IDX = vocab_src['<pad>']\n","START_IDX = vocab_src['<start>']\n","END_IDX = vocab_src['<end>']\n","\n","def generate_batch(data_batch):\n","    \n","    batch_src, batch_tgt = [], []\n","    for src, tgt in data_batch:\n","        batch_src.append(src)\n","        batch_tgt.append(tgt)\n","        \n","    batch_src = pad_sequence(batch_src, padding_value=PAD_IDX)\n","    batch_tgt = pad_sequence(batch_tgt, padding_value=PAD_IDX)\n","    \n","    return batch_src, batch_tgt\n","\n","train_iter = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n","valid_iter = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["8000"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["len(train_data)"]},{"cell_type":"markdown","metadata":{},"source":["Transoformerの設定"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["class TokenEmbedding(nn.Module):\n","    \n","    def __init__(self, vocab_size, embedding_size):\n","        \n","        super(TokenEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_size)\n","        self.embedding_size = embedding_size\n","        \n","    def forward(self, tokens: Tensor):\n","        return self.embedding(tokens.long()) * math.sqrt(self.embedding_size)\n","    \n","    \n","class PositionalEncoding(nn.Module):\n","    \n","    def __init__(self, embedding_size: int, dropout: float, maxlen: int = 5000):\n","        super(PositionalEncoding, self).__init__()\n","        \n","        den = torch.exp(-torch.arange(0, embedding_size, 2) * math.log(10000) / embedding_size)\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        embedding_pos = torch.zeros((maxlen, embedding_size))\n","        embedding_pos[:, 0::2] = torch.sin(pos * den)\n","        embedding_pos[:, 1::2] = torch.cos(pos * den)\n","        embedding_pos = embedding_pos.unsqueeze(-2)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('embedding_pos', embedding_pos)\n","\n","    def forward(self, token_embedding: Tensor):\n","        return self.dropout(token_embedding + self.embedding_pos[: token_embedding.size(0), :])\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["\n","class Seq2SeqTransformer(nn.Module):\n","    \n","    def __init__(\n","        self, num_encoder_layers: int, num_decoder_layers: int,\n","        embedding_size: int, vocab_size_src: int, vocab_size_tgt: int,\n","        dim_feedforward:int = 512, dropout:float = 0.1, nhead:int = 8\n","    ):\n","        \n","        super(Seq2SeqTransformer, self).__init__()\n","\n","        self.token_embedding_src = TokenEmbedding(vocab_size_src, embedding_size)\n","        self.positional_encoding = PositionalEncoding(embedding_size, dropout=dropout)\n","        encoder_layer = TransformerEncoderLayer(\n","            d_model=embedding_size, nhead=nhead, dim_feedforward=dim_feedforward\n","        )\n","        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n","        \n","        self.token_embedding_tgt = TokenEmbedding(vocab_size_tgt, embedding_size)\n","        decoder_layer = TransformerDecoderLayer(\n","            d_model=embedding_size, nhead=nhead, dim_feedforward=dim_feedforward\n","        )\n","        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n","        \n","        self.output = nn.Linear(embedding_size, vocab_size_tgt)\n","\n","    def forward(\n","        self, src: Tensor, tgt: Tensor,\n","        mask_src: Tensor, mask_tgt: Tensor,\n","        padding_mask_src: Tensor, padding_mask_tgt: Tensor,\n","        memory_key_padding_mask: Tensor\n","    ):\n","        \n","        embedding_src = self.positional_encoding(self.token_embedding_src(src))\n","        memory = self.transformer_encoder(embedding_src, mask_src, padding_mask_src)\n","        embedding_tgt = self.positional_encoding(self.token_embedding_tgt(tgt))\n","        outs = self.transformer_decoder(\n","            embedding_tgt, memory, mask_tgt, None,\n","            padding_mask_tgt, memory_key_padding_mask\n","        )\n","        return self.output(outs)\n","\n","    def encode(self, src: Tensor, mask_src: Tensor):\n","        return self.transformer_encoder(self.positional_encoding(self.token_embedding_src(src)), mask_src)\n","\n","    def decode(self, tgt: Tensor, memory: Tensor, mask_tgt: Tensor):\n","        return self.transformer_decoder(self.positional_encoding(self.token_embedding_tgt(tgt)), memory, mask_tgt)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def create_mask(src, tgt, PAD_IDX):\n","    \n","    seq_len_src = src.shape[0]\n","    seq_len_tgt = tgt.shape[0]\n","\n","    mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n","    mask_tgt = generate_square_subsequent_mask(seq_len_tgt)\n","\n","    padding_mask_src = (src == PAD_IDX).transpose(0, 1)\n","    padding_mask_tgt = (tgt == PAD_IDX).transpose(0, 1)\n","    \n","    return mask_src, mask_tgt, padding_mask_src, padding_mask_tgt\n","\n","\n","def generate_square_subsequent_mask(seq_len):\n","    mask = (torch.triu(torch.ones((seq_len, seq_len), device=device)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    return mask"]},{"cell_type":"markdown","metadata":{},"source":["学習の定義"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def train(model, data, optimizer, criterion, PAD_IDX):\n","    \n","    model.train()\n","    losses = 0\n","    for src, tgt in tqdm(data):\n","        \n","        src = src.to(device)\n","        tgt = tgt.to(device)\n","\n","        input_tgt = tgt[:-1, :]\n","\n","        mask_src, mask_tgt, padding_mask_src, padding_mask_tgt = create_mask(src, input_tgt, PAD_IDX)\n","\n","        logits = model(\n","            src=src, tgt=input_tgt,\n","            mask_src=mask_src, mask_tgt=mask_tgt,\n","            padding_mask_src=padding_mask_src, padding_mask_tgt=padding_mask_tgt,\n","            memory_key_padding_mask=padding_mask_src\n","        )\n","\n","        optimizer.zero_grad()\n","        output_tgt = tgt[1:, :]\n","        loss = criterion(logits.reshape(-1, logits.shape[-1]), output_tgt.reshape(-1))\n","        loss.backward()\n","\n","        optimizer.step()\n","        losses += loss.item()\n","        \n","    return losses / len(data)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["\n","def evaluate(model, data, criterion, PAD_IDX):\n","    \n","    model.eval()\n","    losses = 0\n","    for src, tgt in data:\n","        \n","        src = src.to(device)\n","        tgt = tgt.to(device)\n","\n","        input_tgt = tgt[:-1, :]\n","\n","        mask_src, mask_tgt, padding_mask_src, padding_mask_tgt = create_mask(src, input_tgt, PAD_IDX)\n","\n","        logits = model(\n","            src=src, tgt=input_tgt,\n","            mask_src=mask_src, mask_tgt=mask_tgt,\n","            padding_mask_src=padding_mask_src, padding_mask_tgt=padding_mask_tgt,\n","            memory_key_padding_mask=padding_mask_src\n","        )\n","        \n","        output_tgt = tgt[1:, :]\n","        loss = criterion(logits.reshape(-1, logits.shape[-1]), output_tgt.reshape(-1))\n","        losses += loss.item()\n","        \n","    return losses / len(data)"]},{"cell_type":"markdown","metadata":{},"source":["設定"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/arifuku/ymmtlab/TransformerAnsys/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]}],"source":["vocab_size_src = len(vocab_src)\n","vocab_size_tgt = len(vocab_tgt)\n","embedding_size = 4\n","nhead = 1\n","dim_feedforward = 4\n","num_encoder_layers = 1\n","num_decoder_layers = 1\n","dropout = 0.1\n","# vocab_size_src = len(vocab_src)\n","# vocab_size_tgt = len(vocab_tgt)\n","# embedding_size = 240\n","# nhead = 8\n","# dim_feedforward = 100\n","# num_encoder_layers = 2\n","# num_decoder_layers = 2\n","# dropout = 0.1\n","\n","model = Seq2SeqTransformer(\n","    num_encoder_layers=num_encoder_layers,\n","    num_decoder_layers=num_decoder_layers,\n","    embedding_size=embedding_size,\n","    vocab_size_src=vocab_size_src, vocab_size_tgt=vocab_size_tgt,\n","    dim_feedforward=dim_feedforward,\n","    dropout=dropout, nhead=nhead\n",")\n","\n","for p in model.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","model = model.to(device)\n","\n","criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","\n","optimizer = torch.optim.Adam(model.parameters())"]},{"cell_type":"markdown","metadata":{},"source":["モデルの調査"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Seq2SeqTransformer(\n","  (token_embedding_src): TokenEmbedding(\n","    (embedding): Embedding(15, 4)\n","  )\n","  (positional_encoding): PositionalEncoding(\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (transformer_encoder): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n","        )\n","        (linear1): Linear(in_features=4, out_features=4, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (linear2): Linear(in_features=4, out_features=4, bias=True)\n","        (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (token_embedding_tgt): TokenEmbedding(\n","    (embedding): Embedding(14, 4)\n","  )\n","  (transformer_decoder): TransformerDecoder(\n","    (layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n","        )\n","        (multihead_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n","        )\n","        (linear1): Linear(in_features=4, out_features=4, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (linear2): Linear(in_features=4, out_features=4, bias=True)\n","        (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n","        (norm3): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","        (dropout3): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (output): Linear(in_features=4, out_features=14, bias=True)\n",")\n"]}],"source":["print(model)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["34 層\n","\n","層名: token_embedding_src.embedding.weight\n","形状: torch.Size([15, 4])\n","値: Parameter containing:\n","tensor([[-0.3925,  0.0219, -0.1580,  0.1845],\n","        [ 0.4732,  0.5318, -0.3634, -0.2311],\n","        [-0.2304,  0.5460,  0.0412, -0.3118],\n","        [-0.0926, -0.1232,  0.0599,  0.3681],\n","        [-0.1043,  0.4852,  0.3944, -0.4721],\n","        [-0.0690, -0.0945, -0.2168, -0.4495],\n","        [ 0.3925,  0.5431,  0.3693, -0.3931],\n","        [-0.4454,  0.2767,  0.2838,  0.3663],\n","        [-0.4032,  0.3770, -0.1417, -0.0224],\n","        [-0.3853, -0.0826, -0.4591, -0.2396],\n","        [-0.4310,  0.3241,  0.0065,  0.2934],\n","        [-0.2620,  0.2917, -0.3118,  0.0955],\n","        [-0.3276,  0.4564, -0.0085, -0.1193],\n","        [-0.1920, -0.4846,  0.4985, -0.2917],\n","        [-0.0965,  0.1863, -0.2177,  0.2027]], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_encoder.layers.0.self_attn.in_proj_weight\n","形状: torch.Size([12, 4])\n","値: Parameter containing:\n","tensor([[-0.2927, -0.5511, -0.5008,  0.4774],\n","        [ 0.1951,  0.0180, -0.0606, -0.3836],\n","        [ 0.4514,  0.4832, -0.5845,  0.0942],\n","        [ 0.0034,  0.4343,  0.0702,  0.0586],\n","        [ 0.2303,  0.1826,  0.3474,  0.0757],\n","        [-0.4773, -0.5184, -0.3683, -0.1589],\n","        [-0.5318,  0.0861, -0.0586,  0.3740],\n","        [ 0.4095, -0.2751, -0.2163,  0.1615],\n","        [ 0.1226, -0.1553,  0.0225, -0.0785],\n","        [-0.5785,  0.3214,  0.2546, -0.0384],\n","        [ 0.0164, -0.1550,  0.4111, -0.2650],\n","        [-0.1109,  0.2057,  0.0768,  0.0050]], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_encoder.layers.0.self_attn.in_proj_bias\n","形状: torch.Size([12])\n","値: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_encoder.layers.0.self_attn.out_proj.weight\n","形状: torch.Size([4, 4])\n","値: Parameter containing:\n","tensor([[-0.8106,  0.5915, -0.2814,  0.7371],\n","        [-0.6022, -0.7153,  0.3424,  0.1171],\n","        [-0.0918,  0.4288,  0.2097, -0.2772],\n","        [-0.3500, -0.1624, -0.1789,  0.7004]], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_encoder.layers.0.self_attn.out_proj.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_encoder.layers.0.linear1.weight\n","形状: torch.Size([4, 4])\n","値: Parameter containing:\n","tensor([[ 0.7622,  0.7451, -0.6212, -0.4632],\n","        [-0.7724,  0.8581, -0.5526,  0.7368],\n","        [ 0.8358,  0.1571, -0.3381, -0.1952],\n","        [-0.5082, -0.3169, -0.2025, -0.8210]], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_encoder.layers.0.linear1.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([ 0.4043, -0.3179,  0.2952,  0.0622], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_encoder.layers.0.linear2.weight\n","形状: torch.Size([4, 4])\n","値: Parameter containing:\n","tensor([[ 0.2575,  0.0103,  0.3955,  0.2017],\n","        [-0.5443,  0.6297, -0.0824, -0.5303],\n","        [ 0.2958,  0.5787,  0.1398,  0.3568],\n","        [-0.0728, -0.4499,  0.2490, -0.0490]], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_encoder.layers.0.linear2.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([ 0.3830, -0.4522, -0.4206, -0.1047], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_encoder.layers.0.norm1.weight\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([1., 1., 1., 1.], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_encoder.layers.0.norm1.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_encoder.layers.0.norm2.weight\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([1., 1., 1., 1.], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_encoder.layers.0.norm2.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","\n","層名: token_embedding_tgt.embedding.weight\n","形状: torch.Size([14, 4])\n","値: Parameter containing:\n","tensor([[ 0.3704, -0.4917, -0.4534,  0.4176],\n","        [ 0.3072,  0.2775,  0.0197, -0.4879],\n","        [ 0.0478, -0.3828, -0.0461,  0.1011],\n","        [ 0.1640,  0.1194,  0.3150,  0.0178],\n","        [-0.1397, -0.4686,  0.0117, -0.0561],\n","        [ 0.2128, -0.2978,  0.5057, -0.2112],\n","        [ 0.5687, -0.2069, -0.2026, -0.0201],\n","        [-0.1281,  0.2082,  0.3393,  0.3045],\n","        [-0.1226, -0.3292, -0.5281,  0.0207],\n","        [ 0.1530, -0.0493,  0.3088, -0.1368],\n","        [-0.3543, -0.0916,  0.0906, -0.4648],\n","        [-0.2843,  0.0362, -0.5121, -0.4131],\n","        [ 0.5251, -0.0549,  0.3745,  0.1725],\n","        [ 0.2414, -0.1557,  0.2794, -0.5650]], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_decoder.layers.0.self_attn.in_proj_weight\n","形状: torch.Size([12, 4])\n","値: Parameter containing:\n","tensor([[-0.2002,  0.0501, -0.5151, -0.6049],\n","        [-0.2768, -0.2959, -0.3167,  0.5842],\n","        [-0.4064, -0.1411,  0.3623,  0.0798],\n","        [ 0.3371,  0.5138,  0.0766,  0.4726],\n","        [-0.5160, -0.0748,  0.5590, -0.0862],\n","        [-0.0777, -0.0128,  0.1112,  0.3035],\n","        [ 0.3828,  0.1643,  0.3540,  0.4662],\n","        [ 0.1440,  0.4000, -0.2765,  0.4692],\n","        [ 0.3046,  0.5996, -0.5300,  0.2198],\n","        [-0.3290,  0.3027, -0.5247, -0.2984],\n","        [-0.4690, -0.3730, -0.3604,  0.4086],\n","        [-0.2237,  0.3727, -0.0815, -0.0793]], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_decoder.layers.0.self_attn.in_proj_bias\n","形状: torch.Size([12])\n","値: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_decoder.layers.0.self_attn.out_proj.weight\n","形状: torch.Size([4, 4])\n","値: Parameter containing:\n","tensor([[-0.8636,  0.3198,  0.7325, -0.0813],\n","        [ 0.7396,  0.7965,  0.8159,  0.7520],\n","        [ 0.3075, -0.7412,  0.3373,  0.7343],\n","        [ 0.1750, -0.6198,  0.5654, -0.6812]], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_decoder.layers.0.self_attn.out_proj.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_decoder.layers.0.multihead_attn.in_proj_weight\n","形状: torch.Size([12, 4])\n","値: Parameter containing:\n","tensor([[ 0.3263, -0.4742,  0.2341, -0.0439],\n","        [ 0.2012,  0.0169, -0.0353, -0.2155],\n","        [-0.0154,  0.1050,  0.3121,  0.4739],\n","        [ 0.0725,  0.2586,  0.0142,  0.2890],\n","        [-0.1479, -0.0337,  0.5342, -0.4223],\n","        [ 0.4412,  0.2414, -0.3503, -0.5152],\n","        [ 0.0766,  0.4916, -0.4127,  0.0151],\n","        [-0.2598, -0.2226, -0.1171, -0.5911],\n","        [-0.3703,  0.5032, -0.1277, -0.1572],\n","        [-0.0218, -0.5552, -0.4213, -0.3289],\n","        [-0.2540, -0.3794, -0.4526,  0.0408],\n","        [-0.6092,  0.0345,  0.1942,  0.4826]], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_decoder.layers.0.multihead_attn.in_proj_bias\n","形状: torch.Size([12])\n","値: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_decoder.layers.0.multihead_attn.out_proj.weight\n","形状: torch.Size([4, 4])\n","値: Parameter containing:\n","tensor([[ 0.7460, -0.5267, -0.6168,  0.7093],\n","        [-0.4863, -0.7061, -0.0624, -0.3787],\n","        [ 0.4818,  0.6671, -0.0837, -0.7324],\n","        [ 0.1482,  0.3505, -0.7400, -0.0581]], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_decoder.layers.0.multihead_attn.out_proj.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_decoder.layers.0.linear1.weight\n","形状: torch.Size([4, 4])\n","値: Parameter containing:\n","tensor([[-0.1326, -0.7381, -0.0195, -0.4016],\n","        [ 0.4404, -0.4915,  0.5429,  0.1084],\n","        [ 0.8221, -0.2821,  0.4866, -0.0543],\n","        [ 0.4565,  0.3677,  0.6977, -0.6813]], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_decoder.layers.0.linear1.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([-0.4825, -0.4873, -0.1796,  0.4840], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_decoder.layers.0.linear2.weight\n","形状: torch.Size([4, 4])\n","値: Parameter containing:\n","tensor([[ 0.0759, -0.4298, -0.6983,  0.1345],\n","        [ 0.6227, -0.6458,  0.3161, -0.2613],\n","        [-0.6890,  0.1568, -0.5870,  0.7450],\n","        [ 0.8130, -0.4408,  0.6466, -0.6514]], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_decoder.layers.0.linear2.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([0.1696, 0.0135, 0.1194, 0.2192], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_decoder.layers.0.norm1.weight\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([1., 1., 1., 1.], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_decoder.layers.0.norm1.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_decoder.layers.0.norm2.weight\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([1., 1., 1., 1.], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_decoder.layers.0.norm2.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_decoder.layers.0.norm3.weight\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([1., 1., 1., 1.], device='cuda:0', requires_grad=True)\n","\n","層名: transformer_decoder.layers.0.norm3.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","\n","層名: output.weight\n","形状: torch.Size([14, 4])\n","値: Parameter containing:\n","tensor([[-0.4625, -0.5175,  0.0695,  0.5767],\n","        [-0.5404, -0.3704,  0.1587, -0.0543],\n","        [ 0.5164, -0.0747, -0.5099,  0.1411],\n","        [-0.3342,  0.5501,  0.2698, -0.0235],\n","        [-0.5395, -0.4496,  0.4734,  0.1038],\n","        [ 0.4973,  0.2266, -0.2984, -0.5723],\n","        [-0.3922,  0.2553,  0.1545,  0.0738],\n","        [-0.3722, -0.5476,  0.3893, -0.4605],\n","        [-0.2015, -0.1731, -0.4223, -0.3517],\n","        [-0.3250,  0.5632, -0.0991,  0.5453],\n","        [ 0.2275, -0.0257, -0.2977, -0.3142],\n","        [ 0.0948, -0.0877,  0.0625, -0.5485],\n","        [-0.4224,  0.5564,  0.3904, -0.0586],\n","        [ 0.0135, -0.1136,  0.1209, -0.0170]], device='cuda:0', requires_grad=True)\n","\n","層名: output.bias\n","形状: torch.Size([14])\n","値: Parameter containing:\n","tensor([ 0.3304, -0.4547,  0.0754, -0.1886,  0.2323,  0.4651,  0.3251, -0.2276,  0.0178, -0.2828,\n","        -0.0558, -0.1426,  0.0131,  0.0933], device='cuda:0', requires_grad=True)\n"]}],"source":["# モデル内の層の名前とパラメータ情報を表示\n","LP = list(model.named_parameters())\n","lp = len(LP)\n","print(f\"{lp} 層\")\n","for p in range(0, lp):\n","    print(f\"\\n層名: {LP[p][0]}\")\n","    print(f\"形状: {LP[p][1].shape}\")\n","    print(f\"値: {LP[p][1]}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## 学習実行"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/63 [00:00<?, ?it/s]/home/arifuku/ymmtlab/TransformerAnsys/.venv/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","100%|██████████| 63/63 [00:01<00:00, 47.06it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[1/100] train loss: 2.70, valid loss: 2.48  [1s] counter: 0 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[2/100] train loss: 2.38, valid loss: 2.24  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[3/100] train loss: 2.15, valid loss: 2.04  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 53.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[4/100] train loss: 1.97, valid loss: 1.84  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 53.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[5/100] train loss: 1.79, valid loss: 1.67  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[6/100] train loss: 1.65, valid loss: 1.51  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 53.70it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[7/100] train loss: 1.54, valid loss: 1.41  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 53.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[8/100] train loss: 1.45, valid loss: 1.35  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[9/100] train loss: 1.40, valid loss: 1.31  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[10/100] train loss: 1.37, valid loss: 1.29  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[11/100] train loss: 1.35, valid loss: 1.27  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[12/100] train loss: 1.34, valid loss: 1.26  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[13/100] train loss: 1.33, valid loss: 1.24  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[14/100] train loss: 1.31, valid loss: 1.22  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 53.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[15/100] train loss: 1.29, valid loss: 1.18  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.17it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[16/100] train loss: 1.26, valid loss: 1.14  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 55.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[17/100] train loss: 1.23, valid loss: 1.11  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[18/100] train loss: 1.20, valid loss: 1.07  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 55.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[19/100] train loss: 1.17, valid loss: 1.05  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[20/100] train loss: 1.15, valid loss: 1.03  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.53it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[21/100] train loss: 1.12, valid loss: 1.00  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 55.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[22/100] train loss: 1.09, valid loss: 0.99  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[23/100] train loss: 1.07, valid loss: 0.96  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[24/100] train loss: 1.05, valid loss: 0.94  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[25/100] train loss: 1.03, valid loss: 0.92  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[26/100] train loss: 1.01, valid loss: 0.90  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[27/100] train loss: 0.99, valid loss: 0.87  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[28/100] train loss: 0.97, valid loss: 0.85  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 55.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[29/100] train loss: 0.96, valid loss: 0.84  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[30/100] train loss: 0.94, valid loss: 0.82  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.06it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[31/100] train loss: 0.93, valid loss: 0.79  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.24it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[32/100] train loss: 0.92, valid loss: 0.78  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 53.66it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[33/100] train loss: 0.90, valid loss: 0.76  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 53.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[34/100] train loss: 0.89, valid loss: 0.74  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.52it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[35/100] train loss: 0.87, valid loss: 0.72  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[36/100] train loss: 0.87, valid loss: 0.72  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[37/100] train loss: 0.85, valid loss: 0.70  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 53.79it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[38/100] train loss: 0.86, valid loss: 0.68  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 53.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[39/100] train loss: 0.84, valid loss: 0.67  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[40/100] train loss: 0.83, valid loss: 0.66  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[41/100] train loss: 0.83, valid loss: 0.66  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[42/100] train loss: 0.82, valid loss: 0.65  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[43/100] train loss: 0.81, valid loss: 0.64  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.53it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[44/100] train loss: 0.80, valid loss: 0.63  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 53.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[45/100] train loss: 0.79, valid loss: 0.63  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[46/100] train loss: 0.79, valid loss: 0.62  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 53.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[47/100] train loss: 0.79, valid loss: 0.61  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[48/100] train loss: 0.79, valid loss: 0.61  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[49/100] train loss: 0.79, valid loss: 0.61  [1s] counter: 1 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 53.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[50/100] train loss: 0.77, valid loss: 0.59  [1s] counter: 2 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.62it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[51/100] train loss: 0.78, valid loss: 0.60  [1s] counter: 1 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.17it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[52/100] train loss: 0.77, valid loss: 0.59  [1s] counter: 2 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[53/100] train loss: 0.77, valid loss: 0.59  [1s] counter: 1 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.20it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[54/100] train loss: 0.77, valid loss: 0.58  [1s] counter: 2 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.17it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[55/100] train loss: 0.75, valid loss: 0.58  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 53.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[56/100] train loss: 0.75, valid loss: 0.57  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[57/100] train loss: 0.75, valid loss: 0.56  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.87it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[58/100] train loss: 0.75, valid loss: 0.56  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[59/100] train loss: 0.75, valid loss: 0.56  [1s] counter: 1 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.62it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[60/100] train loss: 0.75, valid loss: 0.56  [1s] counter: 2 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 55.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[61/100] train loss: 0.74, valid loss: 0.55  [1s] counter: 3 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[62/100] train loss: 0.74, valid loss: 0.55  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 55.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[63/100] train loss: 0.73, valid loss: 0.55  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 55.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[64/100] train loss: 0.73, valid loss: 0.54  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[65/100] train loss: 0.72, valid loss: 0.54  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[66/100] train loss: 0.73, valid loss: 0.53  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[67/100] train loss: 0.73, valid loss: 0.55  [1s] counter: 1 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[68/100] train loss: 0.73, valid loss: 0.53  [1s] counter: 2 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[69/100] train loss: 0.72, valid loss: 0.53  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.21it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[70/100] train loss: 0.72, valid loss: 0.54  [1s] counter: 1 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.45it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[71/100] train loss: 0.72, valid loss: 0.52  [1s] counter: 2 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 55.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[72/100] train loss: 0.71, valid loss: 0.54  [1s] counter: 1 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[73/100] train loss: 0.71, valid loss: 0.53  [1s] counter: 2 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 55.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[74/100] train loss: 0.71, valid loss: 0.52  [1s] counter: 3 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[75/100] train loss: 0.71, valid loss: 0.51  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[76/100] train loss: 0.70, valid loss: 0.50  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[77/100] train loss: 0.70, valid loss: 0.52  [1s] counter: 1 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[78/100] train loss: 0.71, valid loss: 0.51  [1s] counter: 2 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 55.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[79/100] train loss: 0.69, valid loss: 0.51  [1s] counter: 3 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 55.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[80/100] train loss: 0.69, valid loss: 0.51  [1s] counter: 4 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.44it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[81/100] train loss: 0.69, valid loss: 0.50  [1s] counter: 5 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 55.04it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[82/100] train loss: 0.68, valid loss: 0.50  [1s] counter: 6 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 55.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[83/100] train loss: 0.68, valid loss: 0.48  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[84/100] train loss: 0.68, valid loss: 0.50  [1s] counter: 1 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.87it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[85/100] train loss: 0.67, valid loss: 0.48  [1s] counter: 2 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[86/100] train loss: 0.67, valid loss: 0.49  [1s] counter: 3 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[87/100] train loss: 0.67, valid loss: 0.50  [1s] counter: 4 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[88/100] train loss: 0.66, valid loss: 0.48  [1s] counter: 5 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 55.06it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[89/100] train loss: 0.68, valid loss: 0.49  [1s] counter: 1 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[90/100] train loss: 0.67, valid loss: 0.48  [1s] counter: 2 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[91/100] train loss: 0.66, valid loss: 0.48  [1s] counter: 3 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[92/100] train loss: 0.66, valid loss: 0.48  [1s] counter: 4 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.52it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[93/100] train loss: 0.65, valid loss: 0.48  [1s] counter: 5 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 53.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[94/100] train loss: 0.65, valid loss: 0.47  [1s] counter: 1 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[95/100] train loss: 0.66, valid loss: 0.47  [1s] counter: 1 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[96/100] train loss: 0.66, valid loss: 0.48  [1s] counter: 2 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.70it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[97/100] train loss: 0.65, valid loss: 0.48  [1s] counter: 3 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[98/100] train loss: 0.65, valid loss: 0.46  [1s] counter: 4 **\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.70it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[99/100] train loss: 0.65, valid loss: 0.48  [1s] counter: 1 \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:01<00:00, 54.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[100/100] train loss: 0.65, valid loss: 0.46  [1s] counter: 2 \n"]}],"source":["epoch = 100\n","best_loss = float('Inf')\n","best_model = None\n","patience = 10\n","counter = 0\n","\n","for loop in range(1, epoch + 1):\n","    \n","    start_time = time.time()\n","    \n","    loss_train = train(\n","        model=model, data=train_iter, optimizer=optimizer,\n","        criterion=criterion, PAD_IDX=PAD_IDX\n","    )\n","    \n","    elapsed_time = time.time() - start_time\n","    \n","    loss_valid = evaluate(\n","        model=model, data=valid_iter, criterion=criterion, PAD_IDX=PAD_IDX\n","    )\n","    \n","    print('[{}/{}] train loss: {:.2f}, valid loss: {:.2f}  [{}{:.0f}s] counter: {} {}'.format(\n","        loop, epoch,\n","        loss_train, loss_valid,\n","        str(int(math.floor(elapsed_time / 60))) + 'm' if math.floor(elapsed_time / 60) > 0 else '',\n","        elapsed_time % 60,\n","        counter,\n","        '**' if best_loss > loss_valid else ''\n","    ))\n","    \n","    if best_loss > loss_valid:\n","        best_loss = loss_valid\n","        best_model = model\n","        counter = 0\n","        \n","    if counter > patience:\n","        break\n","    \n","    counter += 1"]},{"cell_type":"markdown","metadata":{},"source":["学習したモデルの保存"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["torch.save(best_model.state_dict(), model_dir_path.joinpath(version + 'translation_transfomer.pth'))"]},{"cell_type":"markdown","metadata":{},"source":["学習したモデルを使って翻訳をする"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def translate(\n","    model, text, vocab_src, vocab_tgt, seq_len_tgt,\n","    START_IDX, END_IDX\n","):\n","    model.eval()\n","    tokens = convert_text_to_indexes(text, vocab=vocab_src)\n","    num_tokens = len(tokens)\n","\n","    # Tensorに変換\n","    src = torch.LongTensor(tokens).reshape(num_tokens, 1).to(device)\n","    mask_src = torch.zeros((num_tokens, num_tokens), device=device).type(torch.bool)\n","\n","    # デコード\n","    predicts = greedy_decode(\n","        model=model, src=src,\n","        mask_src=mask_src, seq_len_tgt=seq_len_tgt,\n","        START_IDX=START_IDX, END_IDX=END_IDX\n","    ).flatten()\n","\n","    return convert_indexes_to_text(predicts, vocab=vocab_tgt)\n","\n","def greedy_decode(model, src, mask_src, seq_len_tgt, START_IDX, END_IDX):\n","    src = src.to(device)\n","    mask_src = mask_src.to(device)\n","\n","    memory = model.encode(src, mask_src)\n","    ys = torch.ones(1, 1).fill_(START_IDX).type(torch.long).to(device)\n","    \n","    for i in range(seq_len_tgt - 1):\n","        memory = memory.to(device)\n","        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n","        mask_tgt = generate_square_subsequent_mask(ys.size(0)).to(device).type(torch.bool)\n","        \n","        output = model.decode(ys, memory, mask_tgt)\n","        output = output.transpose(0, 1)\n","        output = model.output(output[:, -1])\n","        \n","        # 最も高いスコアのトークンを取得\n","        _, next_word = torch.max(output, dim=1)\n","        next_word = next_word.item()\n","\n","        # 生成されたトークンを追加\n","        ys = torch.cat([ys, torch.ones(1, 1).fill_(next_word).type_as(src.data)], dim=0)\n","        if next_word == END_IDX:\n","            break\n","    \n","    return ys\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Input: 9+1\n","Output: 10\n"]}],"source":["seq_len_tgt = max([len(x[1]) for x in train_data])\n","text = '9+1'\n","\n","# 翻訳を実行\n","translation = translate(\n","    model=best_model, text=text, vocab_src=vocab_src, vocab_tgt=vocab_tgt,\n","    seq_len_tgt=seq_len_tgt,\n","    START_IDX=START_IDX, END_IDX=END_IDX\n",")\n","\n","print(f\"Input: {text}\")\n","print(f\"Output: {translation}\")"]},{"cell_type":"markdown","metadata":{},"source":["## モデルの動作を分析"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/arifuku/ymmtlab/TransformerAnsys/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]},{"data":{"text/plain":["Seq2SeqTransformer(\n","  (token_embedding_src): TokenEmbedding(\n","    (embedding): Embedding(15, 4)\n","  )\n","  (positional_encoding): PositionalEncoding(\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (transformer_encoder): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n","        )\n","        (linear1): Linear(in_features=4, out_features=4, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (linear2): Linear(in_features=4, out_features=4, bias=True)\n","        (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (token_embedding_tgt): TokenEmbedding(\n","    (embedding): Embedding(14, 4)\n","  )\n","  (transformer_decoder): TransformerDecoder(\n","    (layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n","        )\n","        (multihead_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n","        )\n","        (linear1): Linear(in_features=4, out_features=4, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (linear2): Linear(in_features=4, out_features=4, bias=True)\n","        (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n","        (norm3): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","        (dropout3): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (output): Linear(in_features=4, out_features=14, bias=True)\n",")"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","\n","# モデルのロード\n","model_path = model_dir_path.joinpath(version + 'translation_transfomer.pth')\n","loaded_model = Seq2SeqTransformer(\n","    num_encoder_layers=num_encoder_layers,\n","    num_decoder_layers=num_decoder_layers,\n","    embedding_size=embedding_size,\n","    vocab_size_src=vocab_size_src, vocab_size_tgt=vocab_size_tgt,\n","    dim_feedforward=dim_feedforward,\n","    dropout=dropout, nhead=nhead\n",").to(device)\n","loaded_model.load_state_dict(torch.load(model_path))\n","loaded_model.eval()\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def inspect_model_layers(model, src, tgt):\n","    # Token Embedding (src)\n","    src_embedded = model.token_embedding_src(src)\n","    print(\"Source Embedding Shape:\", src_embedded.shape)\n","    \n","    # Token Embedding (tgt)\n","    tgt_embedded = model.token_embedding_tgt(tgt)\n","    print(\"Target Embedding Shape:\", tgt_embedded.shape)\n","    \n","    # Positional Encoding (src)\n","    src_pos_encoded = model.positional_encoding(src_embedded)\n","    print(\"Source Positional Encoding Shape:\", src_pos_encoded.shape)\n","    \n","    # Transformer Encoder Layer\n","    encoder_output = model.transformer_encoder(src_pos_encoded)\n","    print(\"Transformer Encoder Output Shape:\", encoder_output.shape)\n","    \n","    # Transformer Decoder Layer\n","    decoder_output = model.transformer_decoder(tgt_embedded, encoder_output)\n","    print(\"Transformer Decoder Output Shape:\", decoder_output.shape)\n","    \n","    # Final Output\n","    final_output = model.output(decoder_output)\n","    print(\"Final Output Shape:\", final_output.shape)\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Selected Embeddings: tensor([[ 0.8101, -0.0286, -0.3102, -0.0715],\n","        [ 0.7757,  0.4116, -0.1158, -0.3675]], device='cuda:0', grad_fn=<IndexBackward0>)\n","Cosine Similarity between 7 and 6: 0.8128422498703003\n"]}],"source":["def inspect_embedding_similarity(model, indices):\n","    emb = model.token_embedding_src.embedding.weight[indices]\n","    print(\"Selected Embeddings:\", emb)\n","    \n","    # Cosine Similarity\n","    cosine_sim = torch.nn.functional.cosine_similarity(emb[0], emb[1], dim=0)\n","    print(f\"Cosine Similarity between {indices[0]} and {indices[1]}:\", cosine_sim.item())\n","\n","inspect_embedding_similarity(loaded_model, [7, 6])\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 800x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","\n","def visualize_embeddings(model, num_points=10):\n","    embeddings = model.token_embedding_src.embedding.weight[:num_points].cpu().detach().numpy()\n","    \n","    # PCA for 2D visualization\n","    pca = PCA(n_components=2)\n","    embeddings_2d = pca.fit_transform(embeddings)\n","    \n","    plt.figure(figsize=(8, 6))\n","    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n","    for i in range(num_points):\n","        plt.text(embeddings_2d[i, 0], embeddings_2d[i, 1], str(i), fontsize=12)\n","    plt.title(\"Embedding 2D Visualization\")\n","    plt.show()\n","\n","visualize_embeddings(loaded_model)\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'src' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m     v \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlinear(src_pos_encoded, attn_layer\u001b[38;5;241m.\u001b[39min_proj_weight[\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mattn_layer\u001b[38;5;241m.\u001b[39membed_dim:, :], attn_layer\u001b[38;5;241m.\u001b[39min_proj_bias[\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mattn_layer\u001b[38;5;241m.\u001b[39membed_dim:])\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q, k, v\n\u001b[0;32m---> 19\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m inspect_qkv_operations(model, \u001b[43msrc\u001b[49m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, q\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, k\u001b[38;5;241m.\u001b[39mshape)\n","\u001b[0;31mNameError\u001b[0m: name 'src' is not defined"]}],"source":["import torch.nn.functional as F\n","\n","def inspect_qkv_operations(model, src):\n","    src_embedded = model.token_embedding_src(src)\n","    src_pos_encoded = model.positional_encoding(src_embedded)\n","    \n","    # Extract QKV from first Transformer layer\n","    attn_layer = model.transformer_encoder.layers[0].self_attn\n","\n","    print(\"QKV Shape:\", attn_layer.in_proj_weight.shape)\n","    \n","    # Q, K, V are projections of the input, which can be computed as:\n","    q = F.linear(src_pos_encoded, attn_layer.in_proj_weight[:attn_layer.embed_dim, :], attn_layer.in_proj_bias[:attn_layer.embed_dim])\n","    k = F.linear(src_pos_encoded, attn_layer.in_proj_weight[attn_layer.embed_dim:2*attn_layer.embed_dim, :], attn_layer.in_proj_bias[attn_layer.embed_dim:2*attn_layer.embed_dim])\n","    v = F.linear(src_pos_encoded, attn_layer.in_proj_weight[2*attn_layer.embed_dim:, :], attn_layer.in_proj_bias[2*attn_layer.embed_dim:])\n","        \n","    return q, k, v\n","\n","q, k, v = inspect_qkv_operations(model, src)\n","\n","print(\"Q Shape:\", q.shape)\n","print(\"K Shape:\", k.shape)\n","print(\"V Shape:\", v.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["src_text = \"9+3\"\n","src_tensor = torch.tensor(convert_text_to_indexes(src_text, vocab_src)).reshape(-1, 1).to(device)\n","src_embedded = loaded_model.token_embedding_src(src_tensor)\n","\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","# 埋め込みデータをCPU上で取得し、PCAで2次元に縮小\n","embeddings = src_embedded.squeeze(1).cpu().detach().numpy()\n","pca = PCA(n_components=2)\n","embeddings_2d = pca.fit_transform(embeddings)\n","\n","# 2次元マップを表示\n","plt.figure(figsize=(6, 6))\n","plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n","\n","# 埋め込みのインデックスに対応する文字をプロット\n","for i, token in enumerate(src_text):\n","    plt.text(embeddings_2d[i, 0], embeddings_2d[i, 1], token, fontsize=12)\n","\n","plt.title(\"Embedding 2D Visualization for '9+3'\")\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.nn.functional as F\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","def visualize_qkv_2d(model, src_text, vocab_src):\n","    # 数式を埋め込みに変換\n","    src_tensor = torch.tensor(convert_text_to_indexes(src_text, vocab_src)).reshape(-1, 1).to(device)\n","    src_embedded = model.token_embedding_src(src_tensor)\n","    src_pos_encoded = model.positional_encoding(src_embedded)\n","\n","    # エンコーダのAttention LayerからQ, K, Vを取得\n","    attn_layer = model.transformer_encoder.layers[0].self_attn\n","\n","    # Q, K, Vを計算\n","    q = F.linear(src_pos_encoded, attn_layer.in_proj_weight[:attn_layer.embed_dim, :], attn_layer.in_proj_bias[:attn_layer.embed_dim])\n","    k = F.linear(src_pos_encoded, attn_layer.in_proj_weight[attn_layer.embed_dim:2*attn_layer.embed_dim, :], attn_layer.in_proj_bias[attn_layer.embed_dim:2*attn_layer.embed_dim])\n","    v = F.linear(src_pos_encoded, attn_layer.in_proj_weight[2*attn_layer.embed_dim:, :], attn_layer.in_proj_bias[2*attn_layer.embed_dim:])\n","\n","    # Q, K, Vの2次元変換\n","    def plot_2d_projection(embeddings, title):\n","        pca = PCA(n_components=2)\n","        embeddings_2d = pca.fit_transform(embeddings.squeeze(1).cpu().detach().numpy())\n","        plt.figure(figsize=(6, 6))\n","        plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n","        for i, token in enumerate(src_text):\n","            plt.text(embeddings_2d[i, 0], embeddings_2d[i, 1], token, fontsize=12)\n","        plt.title(title)\n","        plt.show()\n","\n","    # Q, K, Vを2次元にプロット\n","    plot_2d_projection(q, \"Q (Query) 2D Projection for '9+3'\")\n","    plot_2d_projection(k, \"K (Key) 2D Projection for '9+3'\")\n","    plot_2d_projection(v, \"V (Value) 2D Projection for '9+3'\")\n","\n","# 9+3の数式に対してQ, K, Vを表示\n","visualize_qkv_2d(loaded_model, '9+3', vocab_src)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"KantaiBERT_2.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
