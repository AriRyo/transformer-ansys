{"cells":[{"cell_type":"markdown","metadata":{"id":"M1oqh0F6W3ad"},"source":["# FomulaBEAT\n","\n","変更点\n","- デコーダのみで学習させる\n","- TransformerDecoderLayerをスクラッチで書く\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["version = '02-3'\n","model_dir = './model/' + version\n","data_path = 'data/eq02.txt'"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2860,"status":"ok","timestamp":1611303247694,"user":{"displayName":"Karan Sonawane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64","userId":"05479461208077736330"},"user_tz":-330},"id":"IMnymRDLe0hi","outputId":"706de1c8-715a-41e2-bdcf-3caa67125bf8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/arifuku/ymmtlab/TransformerAnsys/.venv/lib/python3.10/site-packages/torchtext/data/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/home/arifuku/ymmtlab/TransformerAnsys/.venv/lib/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/home/arifuku/ymmtlab/TransformerAnsys/.venv/lib/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"]}],"source":["from pathlib import Path\n","import math\n","import time\n","from collections import Counter\n","from tqdm import tqdm\n","import torch\n","from torch.utils.data import random_split\n","import torch.nn as nn\n","from torch import Tensor\n","from torch.nn import (\n","    TransformerEncoder, TransformerDecoder,\n","    TransformerEncoderLayer, TransformerDecoderLayer\n",")\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import vocab\n","from torchtext.utils import download_from_url, extract_archive\n","import torch.nn.functional as F\n"]},{"cell_type":"markdown","metadata":{},"source":["パラメータの事前設定"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","torch.set_printoptions(linewidth=100)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","model_dir_path = Path(model_dir)\n","if not model_dir_path.exists():\n","    model_dir_path.mkdir(parents=True)"]},{"cell_type":"markdown","metadata":{},"source":["データの取得"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def read_equation_file(file_path):\n","    with open(file_path, 'r') as file:\n","        lines = file.readlines()\n","    src_data, tgt_data = [], []\n","    for line in lines:\n","        src, tgt = line.strip().split('=')\n","        src_data.append(src)\n","        tgt_data.append(tgt)\n","    return src_data, tgt_data\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['8+0', '5+2', '5+1'] ['8', '7', '6']\n"]}],"source":["# ファイルを読み込み、数式データを取得\n","src_data, tgt_data = read_equation_file(data_path)\n","print(src_data[:3], tgt_data[:3])\n"]},{"cell_type":"markdown","metadata":{},"source":["辞書データの作成"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["\n","SPECIALS = ['<unk>', '<pad>', '<start>', '<end>']\n","\n","def build_vocab(texts):\n","    vocab = {}\n","    idx = 0\n","    # 数字の語彙定義\n","    for i in range(10):\n","        vocab[str(i)] = idx\n","        idx += 1\n","    # 特別語の語彙定義\n","    for sp in SPECIALS:\n","        vocab[sp] = idx\n","        idx += 1\n","    # その他の文字の語彙定義\n","    for text in texts:\n","        for char in text:\n","            if char not in vocab:\n","                vocab[char] = idx\n","                idx += 1\n","    return vocab\n","\n","\n","def convert_text_to_indexes(text, vocab):\n","    # <start> と <end> トークンを追加して数値化\n","    return [vocab['<start>']] + [vocab[char] if char in vocab else vocab['<unk>'] for char in text] + [vocab['<end>']]\n","\n","# データを処理して Train と Valid に分ける関数\n","# データを処理して Train と Valid に分ける関数\n","def data_process_split(src_texts, tgt_texts, vocab_src, vocab_tgt, valid_size=0.2):\n","    # データを数値化\n","    data = []\n","    for (src, tgt) in zip(src_texts, tgt_texts):\n","        src_tensor = torch.tensor(convert_text_to_indexes(src, vocab_src), dtype=torch.long)\n","        tgt_tensor = torch.tensor(convert_text_to_indexes(tgt, vocab_tgt), dtype=torch.long)\n","        data.append((src_tensor, tgt_tensor))\n","    \n","    # データのサイズを計算して、訓練データと検証データに分割\n","    data_size = len(data)\n","    valid_size = int(valid_size * data_size)\n","    train_size = data_size - valid_size\n","\n","    # PyTorchのrandom_splitを使って分割\n","    train_data, valid_data = random_split(data, [train_size, valid_size])\n","    \n","    return train_data, valid_data\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '<unk>': 10, '<pad>': 11, '<start>': 12, '<end>': 13}\n"]}],"source":["# 辞書と逆辞書を構築\n","vocab_src = build_vocab(src_data)\n","vocab_tgt = build_vocab(tgt_data)\n","\n","print(vocab_tgt)"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["インデックス化された文章\n","Input: tensor([12,  9, 14,  1, 13])\n","Output: tensor([12,  1,  0, 13])\n","元に戻した文章\n","Input: 9+1\n","Output: 10\n"]}],"source":["\n","# データを数値化\n","train_data, valid_data = data_process_split(src_data, tgt_data, vocab_src, vocab_tgt)\n","\n","# 結果の確認\n","print('インデックス化された文章')\n","print(f\"Input: {train_data[0][0]}\\nOutput: {train_data[0][1]}\")\n","\n","# インデックスから元の文字列に戻す\n","def convert_indexes_to_text(indexes:list, vocab):\n","    reverse_vocab = {idx: token for token, idx in vocab.items()}\n","    return ''.join([reverse_vocab[idx] for idx in indexes if idx in reverse_vocab and reverse_vocab[idx] not in ['<start>', '<end>', '<pad>']])\n","\n","print('元に戻した文章')\n","print(f\"Input: {convert_indexes_to_text(train_data[0][0].tolist(), vocab_src)}\")\n","print(f\"Output: {convert_indexes_to_text(train_data[0][1].tolist(), vocab_tgt)}\")\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["batch_size = 128\n","PAD_IDX = vocab_src['<pad>']\n","START_IDX = vocab_src['<start>']\n","END_IDX = vocab_src['<end>']\n","\n","def generate_batch(data_batch):\n","    \n","    batch_src, batch_tgt = [], []\n","    for src, tgt in data_batch:\n","        batch_src.append(src)\n","        batch_tgt.append(tgt)\n","        \n","    batch_src = pad_sequence(batch_src, padding_value=PAD_IDX)\n","    batch_tgt = pad_sequence(batch_tgt, padding_value=PAD_IDX)\n","    \n","    return batch_src, batch_tgt\n","\n","train_iter = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n","valid_iter = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["8000"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["len(train_data)"]},{"cell_type":"markdown","metadata":{},"source":["Transoformerの設定"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["class TokenEmbedding(nn.Module):\n","    \n","    def __init__(self, vocab_size, embedding_size):\n","        \n","        super(TokenEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=PAD_IDX)\n","        self.embedding_size = embedding_size\n","        \n","    def forward(self, tokens: Tensor):\n","        return self.embedding(tokens.long()) * math.sqrt(self.embedding_size)\n","    \n","    \n","class PositionalEncoding(nn.Module):\n","    \n","    def __init__(self, embedding_size: int, dropout: float, maxlen: int = 5000):\n","        super(PositionalEncoding, self).__init__()\n","        \n","        den = torch.exp(-torch.arange(0, embedding_size, 2) * math.log(10000) / embedding_size)\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        embedding_pos = torch.zeros((maxlen, embedding_size))\n","        embedding_pos[:, 0::2] = torch.sin(pos * den)\n","        embedding_pos[:, 1::2] = torch.cos(pos * den)\n","        embedding_pos = embedding_pos.unsqueeze(-2)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('embedding_pos', embedding_pos)\n","\n","    def forward(self, token_embedding: Tensor):\n","        return self.dropout(token_embedding + self.embedding_pos[: token_embedding.size(0), :])\n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["\n","class TransformerDecoderLayerScratch(nn.Module):\n","    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n","        super(TransformerDecoderLayerScratch, self).__init__()\n","        # Self-attention for the decoder\n","        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n","        # Multihead attention for attending to encoder outputs (memory)\n","        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n","        # Feedforward layers\n","        self.linear1 = nn.Linear(d_model, dim_feedforward)\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear2 = nn.Linear(dim_feedforward, d_model)\n","        # Layer normalization layers\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.norm3 = nn.LayerNorm(d_model)\n","        # Dropout\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","        self.dropout3 = nn.Dropout(dropout)\n","\n","    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n","        # Self-attention\n","        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)\n","        tgt = tgt + self.dropout1(tgt2)\n","        tgt = self.norm1(tgt)\n","\n","        print('tgt:', tgt.shape)\n","        print('memory:', memory.shape)\n","        print('tgt_mask:', tgt_mask.shape)\n","        \n","        # Attention with the encoder outputs (memory)\n","        tgt2, _ = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)\n","        tgt = tgt + self.dropout2(tgt2)\n","        tgt = self.norm2(tgt)\n","        \n","        # Feedforward network\n","        tgt2 = self.linear2(self.dropout(F.relu(self.linear1(tgt))))\n","        tgt = tgt + self.dropout3(tgt2)\n","        tgt = self.norm3(tgt)\n","\n","        return tgt\n"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["\n","class Seq2SeqTransformer(nn.Module):\n","    \n","    def __init__(\n","        self, num_encoder_layers: int, num_decoder_layers: int,\n","        embedding_size: int, vocab_size_src: int, vocab_size_tgt: int,\n","        dim_feedforward:int = 512, dropout:float = 0.1, nhead:int = 8\n","    ):\n","        \n","        super(Seq2SeqTransformer, self).__init__()\n","\n","        self.token_embedding_src = TokenEmbedding(vocab_size_src, embedding_size)\n","        self.positional_encoding = PositionalEncoding(embedding_size, dropout=dropout)\n","        # encoder_layer = TransformerEncoderLayer(\n","        #     d_model=embedding_size, nhead=nhead, dim_feedforward=dim_feedforward\n","        # )\n","        # self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n","        \n","        self.token_embedding_tgt = TokenEmbedding(vocab_size_tgt, embedding_size)\n","        self.decoder_layer = TransformerDecoderLayerScratch(\n","            d_model=embedding_size, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout\n","        )\n","        # self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n","        \n","        self.output = nn.Linear(embedding_size, vocab_size_tgt)\n","\n","    def forward(\n","        self, src: Tensor, tgt: Tensor,\n","        mask_src: Tensor, mask_tgt: Tensor,\n","        padding_mask_src: Tensor, padding_mask_tgt: Tensor,\n","        memory_key_padding_mask: Tensor\n","    ):\n","        embedding_src = self.positional_encoding(self.token_embedding_src(src))\n","        # memory = self.transformer_encoder(embedding_src, mask_src, padding_mask_src)\n","        embedding_tgt = self.positional_encoding(self.token_embedding_tgt(tgt))\n","        outs = self.decoder_layer(\n","            embedding_tgt, embedding_src, mask_tgt, None,\n","            padding_mask_tgt, memory_key_padding_mask\n","        )\n","        return self.output(outs)\n","\n","    # def encode(self, src: Tensor, mask_src: Tensor):\n","    #     return self.transformer_encoder(self.positional_encoding(self.token_embedding_src(src)), mask_src)\n","\n","    def decode(self, tgt: Tensor, memory: Tensor, mask_tgt: Tensor):\n","        return self.decoder_layer(self.positional_encoding(self.token_embedding_tgt(tgt)), memory, mask_tgt)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def create_mask(src, tgt, PAD_IDX):\n","    \n","    seq_len_src = src.shape[0]\n","    seq_len_tgt = tgt.shape[0]\n","\n","    mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n","    mask_tgt = generate_square_subsequent_mask(seq_len_tgt)\n","\n","    padding_mask_src = (src == PAD_IDX).transpose(0, 1)\n","    padding_mask_tgt = (tgt == PAD_IDX).transpose(0, 1)\n","    \n","    return mask_src, mask_tgt, padding_mask_src, padding_mask_tgt\n","\n","\n","def generate_square_subsequent_mask(seq_len):\n","    mask = (torch.triu(torch.ones((seq_len, seq_len), device=device)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    return mask"]},{"cell_type":"markdown","metadata":{},"source":["学習の定義"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def train(model, data, optimizer, criterion, PAD_IDX):\n","    \n","    model.train()\n","    losses = 0\n","    for src, tgt in tqdm(data):\n","        \n","        src = src.to(device)\n","        tgt = tgt.to(device)\n","\n","        input_tgt = tgt[:-1, :]\n","\n","        mask_src, mask_tgt, padding_mask_src, padding_mask_tgt = create_mask(src, input_tgt, PAD_IDX)\n","\n","        logits = model(\n","            src=src, tgt=input_tgt,\n","            mask_src=mask_src, mask_tgt=mask_tgt,\n","            padding_mask_src=padding_mask_src, padding_mask_tgt=padding_mask_tgt,\n","            memory_key_padding_mask=padding_mask_src\n","        )\n","\n","        optimizer.zero_grad()\n","        output_tgt = tgt[1:, :]\n","        loss = criterion(logits.reshape(-1, logits.shape[-1]), output_tgt.reshape(-1))\n","        loss.backward()\n","\n","        optimizer.step()\n","        losses += loss.item()\n","        \n","    return losses / len(data)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["\n","def evaluate(model, data, criterion, PAD_IDX):\n","    \n","    model.eval()\n","    losses = 0\n","    for src, tgt in data:\n","        \n","        src = src.to(device)\n","        tgt = tgt.to(device)\n","\n","        input_tgt = tgt[:-1, :]\n","\n","        mask_src, mask_tgt, padding_mask_src, padding_mask_tgt = create_mask(src, input_tgt, PAD_IDX)\n","\n","        logits = model(\n","            src=src, tgt=input_tgt,\n","            mask_src=mask_src, mask_tgt=mask_tgt,\n","            padding_mask_src=padding_mask_src, padding_mask_tgt=padding_mask_tgt,\n","            memory_key_padding_mask=padding_mask_src\n","        )\n","        \n","        output_tgt = tgt[1:, :]\n","        loss = criterion(logits.reshape(-1, logits.shape[-1]), output_tgt.reshape(-1))\n","        losses += loss.item()\n","        \n","    return losses / len(data)"]},{"cell_type":"markdown","metadata":{},"source":["設定"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["vocab_size_src = len(vocab_src)\n","vocab_size_tgt = len(vocab_tgt)\n","embedding_size = 4\n","nhead = 1\n","dim_feedforward = 4\n","num_encoder_layers = 1\n","num_decoder_layers = 1\n","dropout = 0\n","# vocab_size_src = len(vocab_src)\n","# vocab_size_tgt = len(vocab_tgt)\n","# embedding_size = 240\n","# nhead = 8\n","# dim_feedforward = 100\n","# num_encoder_layers = 2\n","# num_decoder_layers = 2\n","# dropout = 0.1\n","\n","model = Seq2SeqTransformer(\n","    num_encoder_layers=num_encoder_layers,\n","    num_decoder_layers=num_decoder_layers,\n","    embedding_size=embedding_size,\n","    vocab_size_src=vocab_size_src, vocab_size_tgt=vocab_size_tgt,\n","    dim_feedforward=dim_feedforward,\n","    dropout=dropout, nhead=nhead\n",")\n","\n","for p in model.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","model = model.to(device)\n","\n","criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","\n","optimizer = torch.optim.Adam(model.parameters())"]},{"cell_type":"markdown","metadata":{},"source":["モデルの調査"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Seq2SeqTransformer(\n","  (token_embedding_src): TokenEmbedding(\n","    (embedding): Embedding(15, 4, padding_idx=11)\n","  )\n","  (positional_encoding): PositionalEncoding(\n","    (dropout): Dropout(p=0, inplace=False)\n","  )\n","  (token_embedding_tgt): TokenEmbedding(\n","    (embedding): Embedding(14, 4, padding_idx=11)\n","  )\n","  (decoder_layer): TransformerDecoderLayerScratch(\n","    (self_attn): MultiheadAttention(\n","      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n","    )\n","    (multihead_attn): MultiheadAttention(\n","      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n","    )\n","    (linear1): Linear(in_features=4, out_features=4, bias=True)\n","    (dropout): Dropout(p=0, inplace=False)\n","    (linear2): Linear(in_features=4, out_features=4, bias=True)\n","    (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n","    (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n","    (norm3): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n","    (dropout1): Dropout(p=0, inplace=False)\n","    (dropout2): Dropout(p=0, inplace=False)\n","    (dropout3): Dropout(p=0, inplace=False)\n","  )\n","  (output): Linear(in_features=4, out_features=14, bias=True)\n",")\n"]}],"source":["print(model)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["22 層\n","\n","層名: token_embedding_src.embedding.weight\n","形状: torch.Size([15, 4])\n","値: Parameter containing:\n","tensor([[ 0.1285,  0.1298, -0.0098,  0.1101],\n","        [-0.3056,  0.4552, -0.3491,  0.1926],\n","        [-0.5449, -0.2922,  0.0095, -0.3097],\n","        [ 0.0407, -0.3994,  0.0943, -0.0571],\n","        [-0.1552,  0.2905, -0.4873,  0.2420],\n","        [-0.1616,  0.3126,  0.3157, -0.0112],\n","        [ 0.2244,  0.0142, -0.1428,  0.1542],\n","        [-0.5551,  0.0057,  0.5166, -0.0226],\n","        [-0.1913,  0.1071,  0.4781,  0.3912],\n","        [ 0.1904,  0.1718, -0.1164, -0.0400],\n","        [ 0.4855,  0.2643,  0.3522, -0.3119],\n","        [-0.3131,  0.0244,  0.4814, -0.2200],\n","        [-0.1714,  0.1881, -0.3099,  0.4421],\n","        [-0.1883,  0.4162, -0.4407,  0.4258],\n","        [ 0.3741,  0.3094, -0.0601,  0.0800]], device='cuda:0', requires_grad=True)\n","\n","層名: token_embedding_tgt.embedding.weight\n","形状: torch.Size([14, 4])\n","値: Parameter containing:\n","tensor([[-0.2096,  0.5733, -0.0593,  0.0449],\n","        [ 0.4260,  0.3456, -0.2983, -0.0132],\n","        [ 0.1024,  0.1475, -0.4662, -0.4416],\n","        [-0.3491, -0.5202, -0.1171, -0.2481],\n","        [-0.0023,  0.0665,  0.1519, -0.2843],\n","        [ 0.1921,  0.2471, -0.0875, -0.4158],\n","        [ 0.3169,  0.5706,  0.1819, -0.2028],\n","        [-0.1406, -0.0895, -0.1840, -0.1731],\n","        [ 0.2799,  0.3883, -0.4243, -0.3727],\n","        [-0.0431, -0.3306,  0.1157,  0.2650],\n","        [ 0.5180, -0.5479,  0.1397, -0.5053],\n","        [-0.4307, -0.0955,  0.2650,  0.2594],\n","        [-0.0402, -0.1830, -0.5656,  0.0872],\n","        [ 0.3998, -0.0966,  0.0873,  0.0517]], device='cuda:0', requires_grad=True)\n","\n","層名: decoder_layer.self_attn.in_proj_weight\n","形状: torch.Size([12, 4])\n","値: Parameter containing:\n","tensor([[-0.0348, -0.0080,  0.1765, -0.5672],\n","        [-0.6009,  0.1819,  0.1204,  0.3195],\n","        [-0.2032,  0.2604,  0.5860,  0.3717],\n","        [-0.1517,  0.4938,  0.4288, -0.5049],\n","        [-0.2829, -0.3002, -0.2473,  0.1969],\n","        [ 0.0779, -0.3475,  0.0845, -0.0460],\n","        [-0.1352,  0.3758, -0.0709,  0.2537],\n","        [-0.4016, -0.3801, -0.5457, -0.1276],\n","        [ 0.3159, -0.6004,  0.0340, -0.4751],\n","        [-0.4611, -0.1753,  0.1341, -0.3098],\n","        [ 0.0148,  0.1605, -0.1066,  0.1956],\n","        [-0.3802, -0.5833, -0.2242, -0.3796]], device='cuda:0', requires_grad=True)\n","\n","層名: decoder_layer.self_attn.in_proj_bias\n","形状: torch.Size([12])\n","値: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","\n","層名: decoder_layer.self_attn.out_proj.weight\n","形状: torch.Size([4, 4])\n","値: Parameter containing:\n","tensor([[ 0.6460, -0.5868, -0.0960, -0.1819],\n","        [-0.6263, -0.3366,  0.4267, -0.4725],\n","        [-0.2739,  0.1584,  0.7112, -0.6721],\n","        [ 0.1050, -0.5458,  0.8243,  0.3836]], device='cuda:0', requires_grad=True)\n","\n","層名: decoder_layer.self_attn.out_proj.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","\n","層名: decoder_layer.multihead_attn.in_proj_weight\n","形状: torch.Size([12, 4])\n","値: Parameter containing:\n","tensor([[ 0.2249, -0.2298,  0.4815, -0.2327],\n","        [ 0.2200,  0.1349,  0.0464,  0.2790],\n","        [ 0.0259, -0.0195, -0.2594,  0.3893],\n","        [ 0.0671,  0.2211, -0.4012,  0.1059],\n","        [ 0.1119,  0.0661, -0.0120, -0.0966],\n","        [-0.3320,  0.3292, -0.3484, -0.1503],\n","        [ 0.3405,  0.5004, -0.2908, -0.2762],\n","        [ 0.2442, -0.5230, -0.3857, -0.0130],\n","        [ 0.2501,  0.3618,  0.0247,  0.3802],\n","        [ 0.2728, -0.5660, -0.1601,  0.4131],\n","        [-0.5676,  0.4166, -0.2772,  0.5371],\n","        [-0.3495,  0.1095,  0.5913, -0.0638]], device='cuda:0', requires_grad=True)\n","\n","層名: decoder_layer.multihead_attn.in_proj_bias\n","形状: torch.Size([12])\n","値: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","\n","層名: decoder_layer.multihead_attn.out_proj.weight\n","形状: torch.Size([4, 4])\n","値: Parameter containing:\n","tensor([[ 0.0649, -0.4232, -0.5481, -0.2258],\n","        [-0.8472, -0.4097,  0.4828,  0.7500],\n","        [-0.3822, -0.2090, -0.2001,  0.6831],\n","        [ 0.8644,  0.0546,  0.4003, -0.6435]], device='cuda:0', requires_grad=True)\n","\n","層名: decoder_layer.multihead_attn.out_proj.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","\n","層名: decoder_layer.linear1.weight\n","形状: torch.Size([4, 4])\n","値: Parameter containing:\n","tensor([[ 0.2140,  0.7628, -0.4488,  0.5240],\n","        [-0.3130, -0.2441,  0.3531, -0.0443],\n","        [-0.0291,  0.4388,  0.6448, -0.0496],\n","        [ 0.2671,  0.1855, -0.7107, -0.0440]], device='cuda:0', requires_grad=True)\n","\n","層名: decoder_layer.linear1.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([-0.2553, -0.3191,  0.4147, -0.1097], device='cuda:0', requires_grad=True)\n","\n","層名: decoder_layer.linear2.weight\n","形状: torch.Size([4, 4])\n","値: Parameter containing:\n","tensor([[ 0.7215, -0.3154, -0.6677, -0.4121],\n","        [-0.2243,  0.7235,  0.3625,  0.8169],\n","        [ 0.5729, -0.5796, -0.3148, -0.6888],\n","        [-0.6117,  0.3177,  0.1456,  0.5048]], device='cuda:0', requires_grad=True)\n","\n","層名: decoder_layer.linear2.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([-0.0226,  0.3370, -0.0893,  0.1937], device='cuda:0', requires_grad=True)\n","\n","層名: decoder_layer.norm1.weight\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([1., 1., 1., 1.], device='cuda:0', requires_grad=True)\n","\n","層名: decoder_layer.norm1.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","\n","層名: decoder_layer.norm2.weight\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([1., 1., 1., 1.], device='cuda:0', requires_grad=True)\n","\n","層名: decoder_layer.norm2.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","\n","層名: decoder_layer.norm3.weight\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([1., 1., 1., 1.], device='cuda:0', requires_grad=True)\n","\n","層名: decoder_layer.norm3.bias\n","形状: torch.Size([4])\n","値: Parameter containing:\n","tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","\n","層名: output.weight\n","形状: torch.Size([14, 4])\n","値: Parameter containing:\n","tensor([[-0.0658,  0.4089, -0.3547, -0.4155],\n","        [-0.2682, -0.3889, -0.4319, -0.4264],\n","        [-0.5015, -0.4379, -0.4278,  0.3075],\n","        [-0.2790,  0.2550,  0.5653,  0.3372],\n","        [ 0.5607,  0.5385, -0.1483,  0.2013],\n","        [-0.0780, -0.5309,  0.1619,  0.0826],\n","        [-0.1047,  0.5228,  0.4717,  0.0675],\n","        [-0.4039,  0.5190,  0.1936, -0.1324],\n","        [ 0.3272, -0.5517, -0.3344, -0.5331],\n","        [ 0.2416,  0.4816,  0.0087, -0.2501],\n","        [-0.0374,  0.1725,  0.4085,  0.3372],\n","        [-0.2962,  0.0257,  0.1313, -0.1206],\n","        [-0.1144,  0.2771,  0.0521, -0.5485],\n","        [-0.3998, -0.2313, -0.4950, -0.1501]], device='cuda:0', requires_grad=True)\n","\n","層名: output.bias\n","形状: torch.Size([14])\n","値: Parameter containing:\n","tensor([-0.2692, -0.3194,  0.2937, -0.2131, -0.0119, -0.4808,  0.0524, -0.2155,  0.1515,  0.0935,\n","         0.1574,  0.4049,  0.3421, -0.4129], device='cuda:0', requires_grad=True)\n"]}],"source":["# モデル内の層の名前とパラメータ情報を表示\n","LP = list(model.named_parameters())\n","lp = len(LP)\n","print(f\"{lp} 層\")\n","for p in range(0, lp):\n","    print(f\"\\n層名: {LP[p][0]}\")\n","    print(f\"形状: {LP[p][1].shape}\")\n","    print(f\"値: {LP[p][1]}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## 学習実行"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# epoch = 100\n","# best_loss = float('Inf')\n","# best_model = None\n","# patience = 10\n","# counter = 0\n","\n","# for loop in range(1, epoch + 1):\n","    \n","#     start_time = time.time()\n","    \n","#     loss_train = train(\n","#         model=model, data=train_iter, optimizer=optimizer,\n","#         criterion=criterion, PAD_IDX=PAD_IDX\n","#     )\n","    \n","#     elapsed_time = time.time() - start_time\n","    \n","#     loss_valid = evaluate(\n","#         model=model, data=valid_iter, criterion=criterion, PAD_IDX=PAD_IDX\n","#     )\n","    \n","#     print('[{}/{}] train loss: {:.2f}, valid loss: {:.2f}  [{}{:.0f}s] counter: {} {}'.format(\n","#         loop, epoch,\n","#         loss_train, loss_valid,\n","#         str(int(math.floor(elapsed_time / 60))) + 'm' if math.floor(elapsed_time / 60) > 0 else '',\n","#         elapsed_time % 60,\n","#         counter,\n","#         '**' if best_loss > loss_valid else ''\n","#     ))\n","    \n","#     if best_loss > loss_valid:\n","#         best_loss = loss_valid\n","#         best_model = model\n","#         counter = 0\n","        \n","#     if counter > patience:\n","#         break\n","    \n","#     counter += 1"]},{"cell_type":"markdown","metadata":{},"source":["学習したモデルの保存"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# torch.save(best_model.state_dict(), model_dir_path.joinpath(version + 'translation_transfomer.pth'))"]},{"cell_type":"markdown","metadata":{},"source":["学習したモデルを使って翻訳をする"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# def translate(\n","#     model, text, vocab_src, vocab_tgt, seq_len_tgt,\n","#     START_IDX, END_IDX\n","# ):\n","#     model.eval()\n","#     tokens_src = convert_text_to_indexes(text, vocab=vocab_src)\n","#     num_tokens_src = len(tokens_src)\n","\n","#     # Tensorに変換\n","#     src = torch.LongTensor(tokens_src).reshape(num_tokens_src, 1).to(device)\n","#     mask_src = torch.zeros((num_tokens_src, num_tokens_src), device=device).type(torch.bool)\n","\n","#     # デコード\n","#     predicts = greedy_decode(\n","#         model=model, src=src,\n","#         mask_src=mask_src, seq_len_tgt=seq_len_tgt,\n","#         START_IDX=START_IDX, END_IDX=END_IDX\n","#     ).flatten()\n","\n","#     return convert_indexes_to_text(predicts, vocab=vocab_tgt)\n","\n","# def greedy_decode(model, src, mask_src, seq_len_tgt, START_IDX, END_IDX):\n","#     src = src.to(device)\n","#     mask_src = mask_src.to(device)\n","\n","#     # ソースの埋め込みをメモリとして利用\n","#     memory = model.positional_encoding(model.token_embedding_src(src))\n","    \n","#     ys = torch.ones(1, 1).fill_(START_IDX).type(torch.long).to(device)\n","    \n","#     for i in range(seq_len_tgt - 1):\n","#         memory = memory.to(device)\n","#         mask_tgt = generate_square_subsequent_mask(ys.size(0)).to(device).type(torch.bool)\n","        \n","#         output = model.decode(ys, memory, mask_tgt)\n","#         output = output.transpose(0, 1)\n","#         output = model.output(output[:, -1])\n","        \n","#         # 最も高いスコアのトークンを取得\n","#         _, next_word = torch.max(output, dim=1)\n","#         next_word = next_word.item()\n","\n","#         # 生成されたトークンを追加\n","#         ys = torch.cat([ys, torch.ones(1, 1).fill_(next_word).type_as(src.data)], dim=0)\n","#         if next_word == END_IDX:\n","#             break\n","    \n","#     return ys\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# seq_len_tgt = max([len(x[1]) for x in train_data])\n","# text = '4+3'\n","\n","# # 翻訳を実行\n","# translation = translate(\n","#     model=best_model, text=text, vocab_src=vocab_src, vocab_tgt=vocab_tgt,\n","#     seq_len_tgt=seq_len_tgt,\n","#     START_IDX=START_IDX, END_IDX=END_IDX\n","# )\n","\n","# print(f\"Input: {text}\")\n","# print(f\"Output: {translation}\")"]},{"cell_type":"markdown","metadata":{},"source":["## モデルの動作を分析"]},{"cell_type":"code","execution_count":131,"metadata":{},"outputs":[{"data":{"text/plain":["Seq2SeqTransformer(\n","  (token_embedding_src): TokenEmbedding(\n","    (embedding): Embedding(15, 4, padding_idx=11)\n","  )\n","  (positional_encoding): PositionalEncoding(\n","    (dropout): Dropout(p=0, inplace=False)\n","  )\n","  (token_embedding_tgt): TokenEmbedding(\n","    (embedding): Embedding(14, 4, padding_idx=11)\n","  )\n","  (decoder_layer): TransformerDecoderLayerScratch(\n","    (self_attn): MultiheadAttention(\n","      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n","    )\n","    (multihead_attn): MultiheadAttention(\n","      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n","    )\n","    (linear1): Linear(in_features=4, out_features=4, bias=True)\n","    (dropout): Dropout(p=0, inplace=False)\n","    (linear2): Linear(in_features=4, out_features=4, bias=True)\n","    (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n","    (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n","    (norm3): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n","    (dropout1): Dropout(p=0, inplace=False)\n","    (dropout2): Dropout(p=0, inplace=False)\n","    (dropout3): Dropout(p=0, inplace=False)\n","  )\n","  (output): Linear(in_features=4, out_features=14, bias=True)\n",")"]},"execution_count":131,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","\n","# モデルのロード\n","model_path = model_dir_path.joinpath(version + 'translation_transfomer.pth')\n","loaded_model = Seq2SeqTransformer(\n","    num_encoder_layers=num_encoder_layers,\n","    num_decoder_layers=num_decoder_layers,\n","    embedding_size=embedding_size,\n","    vocab_size_src=vocab_size_src, vocab_size_tgt=vocab_size_tgt,\n","    dim_feedforward=dim_feedforward,\n","    dropout=dropout, nhead=nhead\n",").to(device)\n","loaded_model.load_state_dict(torch.load(model_path))\n","loaded_model.eval()\n"]},{"cell_type":"code","execution_count":132,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['token_embedding_src.embedding.weight', 'token_embedding_tgt.embedding.weight', 'decoder_layer.self_attn.in_proj_weight', 'decoder_layer.self_attn.in_proj_bias', 'decoder_layer.self_attn.out_proj.weight', 'decoder_layer.self_attn.out_proj.bias', 'decoder_layer.multihead_attn.in_proj_weight', 'decoder_layer.multihead_attn.in_proj_bias', 'decoder_layer.multihead_attn.out_proj.weight', 'decoder_layer.multihead_attn.out_proj.bias', 'decoder_layer.linear1.weight', 'decoder_layer.linear1.bias', 'decoder_layer.linear2.weight', 'decoder_layer.linear2.bias', 'decoder_layer.norm1.weight', 'decoder_layer.norm1.bias', 'decoder_layer.norm2.weight', 'decoder_layer.norm2.bias', 'decoder_layer.norm3.weight', 'decoder_layer.norm3.bias', 'output.weight', 'output.bias'])\n"]}],"source":["# パラメータを取り出す\n","\n","\n","# モデルのパラメータを取得\n","params = dict(loaded_model.named_parameters())\n","print(params.keys())\n","\n","# 埋め込み行列を取得\n","embedding_src_weight = params['token_embedding_src.embedding.weight'].data\n","embedding_tgt_weight = params['token_embedding_tgt.embedding.weight'].data\n","\n","# 線形層の重みとバイアス\n","output_weight = params['output.weight'].data\n","output_bias = params['output.bias'].data\n","\n","# デコーダの自己注意の重みとバイアス\n","self_attn_in_proj_weight = params['decoder_layer.self_attn.in_proj_weight'].data\n","self_attn_in_proj_bias = params['decoder_layer.self_attn.in_proj_bias'].data\n","self_attn_out_proj_weight = params['decoder_layer.self_attn.out_proj.weight'].data\n","self_attn_out_proj_bias = params['decoder_layer.self_attn.out_proj.bias'].data\n","\n","# メモリー注意の重みとバイアス\n","multihead_attn_in_proj_weight = params['decoder_layer.multihead_attn.in_proj_weight'].data\n","multihead_attn_in_proj_bias = params['decoder_layer.multihead_attn.in_proj_bias'].data\n","multihead_attn_out_proj_weight = params['decoder_layer.multihead_attn.out_proj.weight'].data\n","multihead_attn_out_proj_bias = params['decoder_layer.multihead_attn.out_proj.bias'].data\n","\n","# フィードフォワードネットワークの重みとバイアス\n","linear1_weight = params['decoder_layer.linear1.weight'].data\n","linear1_bias = params['decoder_layer.linear1.bias'].data\n","linear2_weight = params['decoder_layer.linear2.weight'].data\n","linear2_bias = params['decoder_layer.linear2.bias'].data\n","\n","# LayerNormのパラメータ\n","norm1_weight = params['decoder_layer.norm1.weight'].data\n","norm1_bias = params['decoder_layer.norm1.bias'].data\n","norm2_weight = params['decoder_layer.norm2.weight'].data\n","norm2_bias = params['decoder_layer.norm2.bias'].data\n","norm3_weight = params['decoder_layer.norm3.weight'].data\n","norm3_bias = params['decoder_layer.norm3.bias'].data\n"]},{"cell_type":"code","execution_count":133,"metadata":{},"outputs":[],"source":["\n","# Positional Encoding\n","def positional_encoding(tensor: Tensor, maxlen=5000):\n","    embedding_size = tensor.size(-1)\n","    den = torch.exp(-torch.arange(0, embedding_size, 2) * math.log(10000) / embedding_size)\n","    pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","    embedding_pos = torch.zeros((maxlen, embedding_size))\n","    embedding_pos[:, 0::2] = torch.sin(pos * den)\n","    embedding_pos[:, 1::2] = torch.cos(pos * den)\n","    embedding_pos = embedding_pos.unsqueeze(-2)\n","    return tensor + embedding_pos[: tensor.size(0), :].to(tensor.device)"]},{"cell_type":"code","execution_count":169,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[  8.0768,   8.1649, -11.4503, -21.2393, -22.3936, -15.2151,  -2.2355,   9.7388,   2.5360,\n","          -0.7041,   0.1431,  -0.1674,   0.4346,  18.2473]], device='cuda:0',\n","       grad_fn=<AddmmBackward0>)\n","tensor([18.2473], device='cuda:0', grad_fn=<SelectBackward0>)\n","4\n","tensor([[  1.5946,  -5.9857,   7.7078,  18.7305,  22.7645,  19.4159,   8.7318, -10.0034, -15.7477,\n","         -16.7061,  -0.6714,   0.3011,   1.5128, -10.4346]], device='cuda:0',\n","       grad_fn=<AddmmBackward0>)\n","tensor([-10.4346], device='cuda:0', grad_fn=<SelectBackward0>)\n","13\n","Input: 4+0\n","Decoded sequence: 4\n"]}],"source":["\n","# 翻訳処理を実行\n","seq_len_tgt = max([len(x[1]) for x in train_data])\n","text = '4+0'\n","\n","tokens_src = convert_text_to_indexes(text, vocab=vocab_src)\n","src = torch.LongTensor(tokens_src).reshape(len(tokens_src), 1).to(device)\n","memory = positional_encoding(embedding_src_weight[src] * math.sqrt(embedding_size))\n","ys = torch.ones(1, 1).fill_(START_IDX).type(torch.long).to(device)\n","\n","for i in range(10):\n","    tgt_embed = positional_encoding(embedding_tgt_weight[ys] * math.sqrt(embedding_size))\n","    tgt_mask = generate_square_subsequent_mask(ys.size(0)).to(device).type(torch.bool)\n","\n","    tgt2, self_attn_weight = loaded_model.decoder_layer.self_attn(tgt_embed, tgt_embed, tgt_embed)\n","    tgt = tgt_embed + tgt2\n","    tgt = loaded_model.decoder_layer.norm1(tgt)\n","\n","    # Attention with the encoder outputs (memory)\n","    tgt2, multi_attn_weight = loaded_model.decoder_layer.multihead_attn(tgt, memory, memory)\n","    tgt = tgt + tgt2\n","    tgt = loaded_model.decoder_layer.norm2(tgt)\n","    \n","    # Feedforward network\n","\n","    # decoder linear1, 2\n","    tgt2 = tgt.matmul(linear1_weight.T) + linear1_bias\n","    tgt2 = F.relu(tgt2)\n","    tgt2 = tgt2.matmul(linear2_weight.T) + linear2_bias\n","    tgt = tgt + tgt2\n","\n","    # LayerNorm\n","    tgt = loaded_model.decoder_layer.norm3(tgt)\n","\n","    print(output)\n","    print(output[: , -1])\n","    output = tgt.transpose(0, 1)\n","    output = loaded_model.output(output[:, -1])\n","\n","    _, next_word = torch.max(output, dim=1)\n","    next_word = next_word.item()\n","\n","    ys = torch.cat([ys, torch.ones(1, 1).fill_(next_word).type_as(src.data)], dim=0)\n","    print(next_word)\n","    \n","    if next_word == END_IDX:\n","        break\n","\n","\n","flat_indexes = [idx for sublist in ys.tolist() for idx in sublist] if isinstance(ys.tolist()[0], list) else ys.tolist()\n","\n","print(f\"Input: {text}\")\n","print(f\"Decoded sequence: {convert_indexes_to_text(flat_indexes, vocab_tgt)}\")"]},{"cell_type":"code","execution_count":135,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'torch.Tensor'>\n","torch.Size([1, 2, 2])\n","tensor([[[0.5330, 0.4670],\n","         [0.4987, 0.5013]]], device='cuda:0', grad_fn=<MeanBackward1>)\n","<class 'torch.Tensor'>\n","torch.Size([1, 2, 5])\n","tensor([[[0.2246, 0.2003, 0.0877, 0.0921, 0.3953],\n","         [0.1011, 0.1382, 0.2386, 0.4479, 0.0742]]], device='cuda:0', grad_fn=<MeanBackward1>)\n"]}],"source":["print(type(self_attn_weight))\n","print(self_attn_weight.shape)\n","print(self_attn_weight)\n","print(type(multi_attn_weight))\n","print(multi_attn_weight.shape)\n","print(multi_attn_weight)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Transformerの検算\n","\n","スクラッチで書くための検算\n","\n","## Multihead Attention\n","\n","Multihead Attentionの動作をスクラッチで書きたいので、ここで検算する\n","\n","参考サイト\n","https://blog.amedama.jp/entry/pytorch-multi-head-attention-verify"]},{"cell_type":"code","execution_count":118,"metadata":{},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n"]},{"cell_type":"code","execution_count":120,"metadata":{},"outputs":[],"source":["edim = 4 # 埋め込み次元\n","num_heads = 1 # ヘッド数\n","model = nn.MultiheadAttention(edim, num_heads, bias=True, batch_first=True)"]},{"cell_type":"code","execution_count":121,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 5, 4])\n","tensor([[[ 0.1554,  1.2360,  1.0639, -0.3720],\n","         [-0.3173, -0.3807, -0.1323,  0.9814],\n","         [ 0.2685,  0.1682,  0.6541, -1.4650],\n","         [-0.5318,  0.3384, -0.7755, -0.8227],\n","         [ 1.0088, -0.0763,  1.5746, -1.5231]],\n","\n","        [[-0.1761,  0.0773, -0.2156,  2.3514],\n","         [-1.1603,  0.2220,  0.6173, -0.0333],\n","         [-0.6132, -1.6530, -0.0637,  0.2078],\n","         [ 0.0238,  0.1894, -0.7039, -0.3044],\n","         [-0.3339,  0.5039,  0.1053, -1.0818]]])\n"]}],"source":["batch_size = 2\n","L=5\n","X = torch.randn(batch_size, L, edim) # 入力\n","\n","Q = K = V = X # クエリ、キー、バリューは全て入力とする\n","print(Q.shape)\n","print(Q)"]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 5, 4])\n","tensor([[[ 0.1358,  0.0877, -0.0675,  0.2377],\n","         [ 0.2223,  0.1380, -0.0535,  0.3154],\n","         [ 0.1997,  0.1280, -0.0665,  0.3035],\n","         [ 0.1347,  0.0820, -0.0598,  0.2552],\n","         [ 0.2354,  0.1519, -0.0651,  0.3224]],\n","\n","        [[-0.0021, -0.0743,  0.0718, -0.0541],\n","         [ 0.0871,  0.0165, -0.0137,  0.0552],\n","         [ 0.1145,  0.0278,  0.0143,  0.0071],\n","         [-0.0942, -0.1198,  0.0600, -0.0701],\n","         [-0.0115, -0.0509,  0.0139, -0.0049]]], grad_fn=<TransposeBackward0>)\n"]}],"source":["\n","attn_output, attn_output_weights = model(Q, K, V)\n","\n","print(attn_output.shape)\n","print(attn_output)\n","\n"]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[('in_proj_weight',\n","  Parameter containing:\n","tensor([[ 0.3135, -0.4200,  0.1098,  0.1657],\n","        [-0.3305, -0.5894,  0.2564, -0.4990],\n","        [ 0.3443,  0.3706, -0.5312, -0.5492],\n","        [-0.0969,  0.0883, -0.5059, -0.2797],\n","        [ 0.2573, -0.5490, -0.0139,  0.4898],\n","        [ 0.5063, -0.0761,  0.1915, -0.6119],\n","        [-0.0861,  0.3069, -0.5451,  0.4586],\n","        [-0.0363, -0.1677,  0.4143,  0.2315],\n","        [-0.0870, -0.4917, -0.1023,  0.3188],\n","        [-0.2304,  0.3617, -0.1264,  0.4168],\n","        [ 0.1843, -0.2980,  0.2385,  0.2357],\n","        [ 0.2304,  0.1970, -0.5439,  0.1361]], requires_grad=True)),\n"," ('in_proj_bias',\n","  Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n"," ('out_proj.weight',\n","  Parameter containing:\n","tensor([[ 0.2063, -0.4731, -0.3537, -0.3683],\n","        [ 0.0500, -0.4336, -0.2108,  0.0188],\n","        [ 0.1191,  0.0621,  0.2418, -0.1074],\n","        [-0.4875,  0.1371,  0.1110, -0.4857]], requires_grad=True)),\n"," ('out_proj.bias',\n","  Parameter containing:\n","tensor([0., 0., 0., 0.], requires_grad=True))]\n"]}],"source":["from pprint import pprint\n","pprint(list(model.named_parameters()))"]},{"cell_type":"code","execution_count":126,"metadata":{},"outputs":[],"source":["model_weight = {name: param.data for name, param in model.named_parameters()}\n","Wi = model_weight['in_proj_weight']\n","Wo = model_weight['out_proj.weight']\n","Wbi = model_weight['in_proj_bias']\n","Wbo = model_weight['out_proj.bias']"]},{"cell_type":"code","execution_count":127,"metadata":{},"outputs":[],"source":["Wi_q, Wi_k, Wi_v = Wi.chunk(3, dim=0)\n","Wbi_q, Wbi_k, Wbi_v = Wbi.chunk(3, dim=0)\n","QW = torch.matmul(Q, Wi_q.T) + Wbi_q\n","KW = torch.matmul(K, Wi_k.T) + Wbi_k\n","VW = torch.matmul(V, Wi_v.T) + Wbi_v\n","\n","KW_t = KW.transpose(-2, -1)\n","QK_t = torch.bmm(QW, KW_t)\n","QK_scaled = QK_t / (edim ** 0.5)\n","attn_weights_ = F.softmax(QK_scaled, dim=-1)"]},{"cell_type":"code","execution_count":128,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[0.2136, 0.1996, 0.1859, 0.2592, 0.1417],\n","         [0.1816, 0.1744, 0.2186, 0.1662, 0.2592],\n","         [0.2044, 0.1549, 0.2161, 0.2081, 0.2165],\n","         [0.2381, 0.2097, 0.1783, 0.2209, 0.1530],\n","         [0.1737, 0.1402, 0.2324, 0.1952, 0.2584]],\n","\n","        [[0.2170, 0.2222, 0.2612, 0.1480, 0.1516],\n","         [0.0793, 0.2149, 0.1782, 0.2091, 0.3185],\n","         [0.0741, 0.1713, 0.2763, 0.1980, 0.2803],\n","         [0.2957, 0.1953, 0.1651, 0.1846, 0.1593],\n","         [0.1836, 0.2054, 0.1493, 0.2246, 0.2371]]])\n","tensor([[[0.2136, 0.1996, 0.1859, 0.2592, 0.1417],\n","         [0.1816, 0.1744, 0.2186, 0.1662, 0.2592],\n","         [0.2044, 0.1549, 0.2161, 0.2081, 0.2165],\n","         [0.2381, 0.2097, 0.1783, 0.2209, 0.1530],\n","         [0.1737, 0.1402, 0.2324, 0.1952, 0.2584]],\n","\n","        [[0.2170, 0.2222, 0.2612, 0.1480, 0.1516],\n","         [0.0793, 0.2149, 0.1782, 0.2091, 0.3185],\n","         [0.0741, 0.1713, 0.2763, 0.1980, 0.2803],\n","         [0.2957, 0.1953, 0.1651, 0.1846, 0.1593],\n","         [0.1836, 0.2054, 0.1493, 0.2246, 0.2371]]], grad_fn=<MeanBackward1>)\n"]}],"source":["print(attn_weights_)\n","print(attn_output_weights)"]},{"cell_type":"code","execution_count":129,"metadata":{},"outputs":[],"source":["AV = torch.matmul(attn_weights_, VW)\n","attn_output_ = torch.matmul(AV, Wo.T) + Wbo"]},{"cell_type":"code","execution_count":130,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[ 0.1358,  0.0877, -0.0675,  0.2377],\n","         [ 0.2223,  0.1380, -0.0535,  0.3154],\n","         [ 0.1997,  0.1280, -0.0665,  0.3035],\n","         [ 0.1347,  0.0820, -0.0598,  0.2552],\n","         [ 0.2354,  0.1519, -0.0651,  0.3224]],\n","\n","        [[-0.0021, -0.0743,  0.0718, -0.0541],\n","         [ 0.0871,  0.0165, -0.0137,  0.0552],\n","         [ 0.1145,  0.0278,  0.0143,  0.0071],\n","         [-0.0942, -0.1198,  0.0600, -0.0701],\n","         [-0.0115, -0.0509,  0.0139, -0.0049]]])\n","tensor([[[ 0.1358,  0.0877, -0.0675,  0.2377],\n","         [ 0.2223,  0.1380, -0.0535,  0.3154],\n","         [ 0.1997,  0.1280, -0.0665,  0.3035],\n","         [ 0.1347,  0.0820, -0.0598,  0.2552],\n","         [ 0.2354,  0.1519, -0.0651,  0.3224]],\n","\n","        [[-0.0021, -0.0743,  0.0718, -0.0541],\n","         [ 0.0871,  0.0165, -0.0137,  0.0552],\n","         [ 0.1145,  0.0278,  0.0143,  0.0071],\n","         [-0.0942, -0.1198,  0.0600, -0.0701],\n","         [-0.0115, -0.0509,  0.0139, -0.0049]]], grad_fn=<TransposeBackward0>)\n"]}],"source":["print(attn_output_)\n","print(attn_output)"]},{"cell_type":"markdown","metadata":{},"source":["## nn.Linear"]},{"cell_type":"code","execution_count":142,"metadata":{},"outputs":[{"data":{"text/plain":["Linear(in_features=4, out_features=4, bias=True)"]},"execution_count":142,"metadata":{},"output_type":"execute_result"}],"source":["model = nn.Linear(4, 4)\n","model"]},{"cell_type":"code","execution_count":143,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[('weight',\n","  Parameter containing:\n","tensor([[-0.3068, -0.1941, -0.0057,  0.2137],\n","        [ 0.2596, -0.1518, -0.3444,  0.1489],\n","        [ 0.3061, -0.2957, -0.3799, -0.2315],\n","        [ 0.2787, -0.1753, -0.1904, -0.0134]], requires_grad=True)),\n"," ('bias',\n","  Parameter containing:\n","tensor([-0.2181,  0.4180,  0.0857, -0.1765], requires_grad=True))]\n"]}],"source":["pprint(list(model.named_parameters()))"]},{"cell_type":"code","execution_count":145,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([4])\n","tensor([ 0.5931,  1.4933, -2.1964, -0.0622])\n","torch.Size([4])\n","tensor([-0.6907,  1.0925,  0.6744,  0.1461], grad_fn=<ViewBackward0>)\n"]}],"source":["model_weight = {name: param.data for name, param in model.named_parameters()}\n","W = model_weight['weight']\n","B = model_weight['bias']\n","\n","X = torch.randn(4) \n","print(X.shape)\n","print(X)\n","output = model(X)\n","print(output.shape)\n","print(output)\n"]},{"cell_type":"code","execution_count":146,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([-0.6907,  1.0925,  0.6744,  0.1461])\n","tensor([-0.6907,  1.0925,  0.6744,  0.1461], grad_fn=<ViewBackward0>)\n"]}],"source":["output_ = X.matmul(W.T) + B\n","print(output_)\n","print(output)"]},{"cell_type":"markdown","metadata":{},"source":["## nn.LayerNorm\n","\n","参考サイト\n","https://qiita.com/dl_from_scratch/items/133fe741b67ed14f1856"]},{"cell_type":"code","execution_count":151,"metadata":{},"outputs":[{"data":{"text/plain":["LayerNorm((4,), eps=1e-05, elementwise_affine=True)"]},"execution_count":151,"metadata":{},"output_type":"execute_result"}],"source":["model = nn.LayerNorm(4)\n","model"]},{"cell_type":"code","execution_count":152,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[('weight', Parameter containing:\n","tensor([1., 1., 1., 1.], requires_grad=True)),\n"," ('bias', Parameter containing:\n","tensor([0., 0., 0., 0.], requires_grad=True))]\n"]}],"source":["pprint(list(model.named_parameters()))"]},{"cell_type":"code","execution_count":153,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([4])\n","tensor([1.1853, 0.6058, 0.7668, 0.0579])\n","torch.Size([4])\n","tensor([ 1.3154, -0.1192,  0.2793, -1.4755], grad_fn=<NativeLayerNormBackward0>)\n"]}],"source":["model_weight = {name: param.data for name, param in model.named_parameters()}\n","W = model_weight['weight']\n","B = model_weight['bias']\n","\n","X = torch.randn(4) \n","print(X.shape)\n","print(X)\n","output = model(X)\n","print(output.shape)\n","print(output)\n"]},{"cell_type":"code","execution_count":154,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([2.6159, 2.6159, 2.6159, 2.6159])\n","tensor([ 1.3154, -0.1192,  0.2793, -1.4755], grad_fn=<NativeLayerNormBackward0>)\n"]}],"source":["output_ = X.matmul(W.T) + B\n","print(output_)\n","print(output)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"KantaiBERT_2.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
